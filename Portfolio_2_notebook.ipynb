{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from os import makedirs\n",
    "from os.path import join, exists\n",
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates two subdirectories called \"theguardian\" and \"collection\"\n",
    "ARTICLES_DIR = join('theguardian', 'collection')\n",
    "makedirs(ARTICLES_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 2019-07-01\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-01.json\n",
      "Downloading 2019-07-02\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-02.json\n",
      "Downloading 2019-07-03\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-03.json\n",
      "Downloading 2019-07-04\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-04.json\n",
      "Downloading 2019-07-05\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-05.json\n",
      "Downloading 2019-07-06\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-07-06.json\n",
      "Downloading 2019-07-07\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-07-07.json\n",
      "Downloading 2019-07-08\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-08.json\n",
      "Downloading 2019-07-09\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-09.json\n",
      "Downloading 2019-07-10\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-10.json\n",
      "Downloading 2019-07-11\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-11.json\n",
      "Downloading 2019-07-12\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-12.json\n",
      "Downloading 2019-07-13\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-07-13.json\n",
      "Downloading 2019-07-14\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-07-14.json\n",
      "Downloading 2019-07-15\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-15.json\n",
      "Downloading 2019-07-16\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-16.json\n",
      "Downloading 2019-07-17\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-17.json\n",
      "Downloading 2019-07-18\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-18.json\n",
      "Downloading 2019-07-19\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-19.json\n",
      "Downloading 2019-07-20\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-07-20.json\n",
      "Downloading 2019-07-21\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-21.json\n",
      "Downloading 2019-07-22\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-22.json\n",
      "Downloading 2019-07-23\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-23.json\n",
      "Downloading 2019-07-24\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-24.json\n",
      "Downloading 2019-07-25\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-25.json\n",
      "Downloading 2019-07-26\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-26.json\n",
      "Downloading 2019-07-27\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-07-27.json\n",
      "Downloading 2019-07-28\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-28.json\n",
      "Downloading 2019-07-29\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-29.json\n",
      "Downloading 2019-07-30\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-30.json\n",
      "Downloading 2019-07-31\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-07-31.json\n",
      "Downloading 2019-08-01\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-08-01.json\n",
      "Downloading 2019-08-02\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-02.json\n",
      "Downloading 2019-08-03\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-08-03.json\n",
      "Downloading 2019-08-04\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-04.json\n",
      "Downloading 2019-08-05\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-05.json\n",
      "Downloading 2019-08-06\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-06.json\n",
      "Downloading 2019-08-07\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-07.json\n",
      "Downloading 2019-08-08\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-08.json\n",
      "Downloading 2019-08-09\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-09.json\n",
      "Downloading 2019-08-10\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-08-10.json\n",
      "Downloading 2019-08-11\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-11.json\n",
      "Downloading 2019-08-12\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-12.json\n",
      "Downloading 2019-08-13\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-13.json\n",
      "Downloading 2019-08-14\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-14.json\n",
      "Downloading 2019-08-15\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-08-15.json\n",
      "Downloading 2019-08-16\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-16.json\n",
      "Downloading 2019-08-17\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-08-17.json\n",
      "Downloading 2019-08-18\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-08-18.json\n",
      "Downloading 2019-08-19\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-19.json\n",
      "Downloading 2019-08-20\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-20.json\n",
      "Downloading 2019-08-21\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-21.json\n",
      "Downloading 2019-08-22\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-22.json\n",
      "Downloading 2019-08-23\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-23.json\n",
      "Downloading 2019-08-24\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-08-24.json\n",
      "Downloading 2019-08-25\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-08-25.json\n",
      "Downloading 2019-08-26\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-08-26.json\n",
      "Downloading 2019-08-27\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-27.json\n",
      "Downloading 2019-08-28\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-28.json\n",
      "Downloading 2019-08-29\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-29.json\n",
      "Downloading 2019-08-30\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-08-30.json\n",
      "Downloading 2019-08-31\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-08-31.json\n",
      "Downloading 2019-09-01\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-01.json\n",
      "Downloading 2019-09-02\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-09-02.json\n",
      "Downloading 2019-09-03\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-03.json\n",
      "Downloading 2019-09-04\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-04.json\n",
      "Downloading 2019-09-05\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-09-05.json\n",
      "Downloading 2019-09-06\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-06.json\n",
      "Downloading 2019-09-07\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-09-07.json\n",
      "Downloading 2019-09-08\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-09-08.json\n",
      "Downloading 2019-09-09\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-09.json\n",
      "Downloading 2019-09-10\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-10.json\n",
      "Downloading 2019-09-11\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-11.json\n",
      "Downloading 2019-09-12\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-12.json\n",
      "Downloading 2019-09-13\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-13.json\n",
      "Downloading 2019-09-14\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-09-14.json\n",
      "Downloading 2019-09-15\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-15.json\n",
      "Downloading 2019-09-16\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-16.json\n",
      "Downloading 2019-09-17\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-17.json\n",
      "Downloading 2019-09-18\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-18.json\n",
      "Downloading 2019-09-19\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-19.json\n",
      "Downloading 2019-09-20\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-20.json\n",
      "Downloading 2019-09-21\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-09-21.json\n",
      "Downloading 2019-09-22\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-09-22.json\n",
      "Downloading 2019-09-23\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-23.json\n",
      "Downloading 2019-09-24\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-24.json\n",
      "Downloading 2019-09-25\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-25.json\n",
      "Downloading 2019-09-26\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-26.json\n",
      "Downloading 2019-09-27\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-27.json\n",
      "Downloading 2019-09-28\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-09-28.json\n",
      "Downloading 2019-09-29\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-09-29.json\n",
      "Downloading 2019-09-30\n",
      "...page 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...page 2\n",
      "Writing to theguardian/collection/2019-09-30.json\n",
      "Downloading 2019-10-01\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-01.json\n",
      "Downloading 2019-10-02\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-02.json\n",
      "Downloading 2019-10-03\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-03.json\n",
      "Downloading 2019-10-04\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-04.json\n",
      "Downloading 2019-10-05\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-10-05.json\n",
      "Downloading 2019-10-06\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-06.json\n",
      "Downloading 2019-10-07\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-07.json\n",
      "Downloading 2019-10-08\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-08.json\n",
      "Downloading 2019-10-09\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-09.json\n",
      "Downloading 2019-10-10\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-10.json\n",
      "Downloading 2019-10-11\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-11.json\n",
      "Downloading 2019-10-12\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-10-12.json\n",
      "Downloading 2019-10-13\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-13.json\n",
      "Downloading 2019-10-14\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-14.json\n",
      "Downloading 2019-10-15\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-15.json\n",
      "Downloading 2019-10-16\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-16.json\n",
      "Downloading 2019-10-17\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-17.json\n",
      "Downloading 2019-10-18\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-18.json\n",
      "Downloading 2019-10-19\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-10-19.json\n",
      "Downloading 2019-10-20\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-20.json\n",
      "Downloading 2019-10-21\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-21.json\n",
      "Downloading 2019-10-22\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-22.json\n",
      "Downloading 2019-10-23\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-23.json\n",
      "Downloading 2019-10-24\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-24.json\n",
      "Downloading 2019-10-25\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-25.json\n",
      "Downloading 2019-10-26\n",
      "...page 1\n",
      "Writing to theguardian/collection/2019-10-26.json\n",
      "Downloading 2019-10-27\n",
      "...page 1\n",
      "...page 2\n",
      "Writing to theguardian/collection/2019-10-27.json\n"
     ]
    }
   ],
   "source": [
    "# Sample URL\n",
    "#\n",
    "# http://content.guardianapis.com/search?from-date=2016-01-02&\n",
    "# to-date=2016-01-02&order-by=newest&show-fields=all&page-size=200\n",
    "# &api-key=your-api-key-goes-here\n",
    "\n",
    "# Change this for your API key:\n",
    "MY_API_KEY = '4b56b50a-8ec9-4b63-a4a9-e7074682df99'\n",
    "\n",
    "API_ENDPOINT = 'http://content.guardianapis.com/search'\n",
    "my_params = {\n",
    "    'from-date': \"\", # leave empty, change start_date / end_date variables instead\n",
    "    'to-date': \"\",\n",
    "    'order-by': \"newest\",\n",
    "    'show-fields': 'all',\n",
    "    'page-size': 200,\n",
    "    'api-key': MY_API_KEY\n",
    "}\n",
    "\n",
    "# day iteration from here:\n",
    "# http://stackoverflow.com/questions/7274267/print-all-day-dates-between-two-dates\n",
    "\n",
    "# Update these dates to suit your own needs.\n",
    "start_date = date(2019, 7, 1)\n",
    "end_date = date(2019,10, 27)\n",
    "\n",
    "dayrange = range((end_date - start_date).days + 1)\n",
    "for daycount in dayrange:\n",
    "    dt = start_date + timedelta(days=daycount)\n",
    "    datestr = dt.strftime('%Y-%m-%d')\n",
    "    fname = join(ARTICLES_DIR, datestr + '.json')\n",
    "    if not exists(fname):\n",
    "        # then let's download it\n",
    "        print(\"Downloading\", datestr)\n",
    "        all_results = []\n",
    "        my_params['from-date'] = datestr\n",
    "        my_params['to-date'] = datestr\n",
    "        current_page = 1\n",
    "        total_pages = 1\n",
    "        while current_page <= total_pages:\n",
    "            print(\"...page\", current_page)\n",
    "            my_params['page'] = current_page\n",
    "            resp = requests.get(API_ENDPOINT, my_params)\n",
    "            data = resp.json()\n",
    "            all_results.extend(data['response']['results'])\n",
    "            # if there is more than one page\n",
    "            current_page += 1\n",
    "            total_pages = data['response']['pages']\n",
    "\n",
    "        with open(fname, 'w') as f:\n",
    "            print(\"Writing to\", fname)\n",
    "\n",
    "            # re-serialize it for pretty indentation\n",
    "            f.write(json.dumps(all_results, indent=2))\n",
    "            \n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids: 25581\n",
      "Number of texts: 25581\n"
     ]
    }
   ],
   "source": [
    "directory_name = \"theguardian/collection/\"\n",
    "\n",
    "ids = list()\n",
    "texts = list()\n",
    "sections = list()\n",
    "for filename in os.listdir(directory_name):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(directory_name + filename) as json_file:\n",
    "            data = json.load(json_file)\n",
    "            for article in data:\n",
    "                id = article['id']\n",
    "                fields = article['fields']\n",
    "                text = fields['bodyText'] if fields['bodyText'] else \"\"\n",
    "                ids.append(id)\n",
    "                texts.append(text)\n",
    "                section = article['sectionId']\t# Id name each article gets by The Guardian\n",
    "                sections.append(section) # Adding each item to a list as above \"sections = list()\"\n",
    "\n",
    "print(\"Number of ids: %d\" % len(ids))\n",
    "print(\"Number of texts: %d\" % len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22332289"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assignment 2\n",
    "# variables : 'texts' is a list of all the texts\n",
    "\n",
    "\n",
    "# describtion of the texts (length etc.):\n",
    "\n",
    "\n",
    "# word count (simple tokenization) \n",
    "word_count = 0\n",
    "for text in texts:\n",
    "    words = text.split()\n",
    "    word_count = word_count + len(words)\n",
    "word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 22332289\n",
      "Unique word count: 589803\n"
     ]
    }
   ],
   "source": [
    "# word count + unique word count\n",
    "all_words = list()\n",
    "for text in texts:\n",
    "  words = text.split()\n",
    "  all_words.extend(words)\n",
    "print(\"Word count: %i\" % len(all_words))\n",
    "unique_words = set(all_words)\n",
    "unique_word_count = len(unique_words)\n",
    "print(\"Unique word count: %i\" % unique_word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/torkellingvarsson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# creating subset of the dataset, our query is for Hong Kong protests, \n",
    "# so I make it only look at news and world news, based on the sections found on the guardians website\n",
    "idxes = []\n",
    "subtexts = []\n",
    "for i, section in enumerate(sections):\n",
    "    if section in ['news','world news',]:\n",
    "        idxes.append(i)\n",
    "        subtexts.append(texts[i])\n",
    "len(idxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'animals-farmed',\n",
       " 'artanddesign',\n",
       " 'australia-news',\n",
       " 'australia-post-building-a-sustainable-future',\n",
       " 'bank-australia-coming-clean',\n",
       " 'best-of-bordeaux',\n",
       " 'books',\n",
       " 'born-adventurous',\n",
       " 'breakthrough-science',\n",
       " 'bringing-classroom-subjects-to-life',\n",
       " 'business',\n",
       " 'business-call-to-action-partnerzone',\n",
       " 'business-to-business',\n",
       " 'careers',\n",
       " 'caresuper-staying-future-focused',\n",
       " 'cgu-rescuing-ambition',\n",
       " 'changemakers-for-a-better-tomorrow',\n",
       " 'cities',\n",
       " 'clinique-beyond-beauty',\n",
       " 'commentisfree',\n",
       " 'community',\n",
       " 'crosswords',\n",
       " 'culture',\n",
       " 'dairy-australia-listen-to-your-gut',\n",
       " 'dairy-australia-sustainable-futures',\n",
       " 'discover-cool-canada',\n",
       " 'diverse-talent-everywhere',\n",
       " 'education',\n",
       " 'environment',\n",
       " 'essex-social-care',\n",
       " 'family-services-barnet',\n",
       " 'fashion',\n",
       " 'fill-your-heart-with-ireland',\n",
       " 'film',\n",
       " 'food',\n",
       " 'football',\n",
       " 'future-energy-skills-know-your-battery',\n",
       " 'games',\n",
       " 'global',\n",
       " 'global-development',\n",
       " 'gnm-press-office',\n",
       " 'gnmeducationcentre',\n",
       " 'guardian-masterclasses',\n",
       " 'help',\n",
       " 'highlights-of-hangzhou',\n",
       " 'inequality',\n",
       " 'info',\n",
       " 'isle-of-wight-healthcare-careers',\n",
       " 'law',\n",
       " 'lifeandstyle',\n",
       " 'live-victoriously',\n",
       " 'media',\n",
       " 'melbourne-international-arts-festival-2019',\n",
       " 'membership',\n",
       " 'modern-basket-tesco',\n",
       " 'modern-healthcare-challenges',\n",
       " 'modern-money',\n",
       " 'money',\n",
       " 'more-than-a-degree',\n",
       " 'music',\n",
       " 'nantes-atlantic-loire-valley',\n",
       " 'new-caledonia-feel-the-pulse',\n",
       " 'new-energy-solar-harness-the-sun',\n",
       " 'news',\n",
       " 'nice-and-cote-d-azur-france',\n",
       " 'politics',\n",
       " 'product-innovation-with-henkel',\n",
       " 'professional-supplements',\n",
       " 'provoking-progress',\n",
       " 'pwc-partner-zone',\n",
       " 'qualified-for-life',\n",
       " 'research-at-the-university-of-nottingham',\n",
       " 'road-safety-commission-streets-ahead',\n",
       " 'science',\n",
       " 'smarter-energy',\n",
       " 'society',\n",
       " 'sounds-of-our-lives',\n",
       " 'spains-secret-coast',\n",
       " 'sport',\n",
       " 'stage',\n",
       " 'tafe-nsw-skills-for-business',\n",
       " 'taking-care-of-business',\n",
       " 'technology',\n",
       " 'the-slopes-are-calling',\n",
       " 'the-source-bulk-foods-shop-sustainable',\n",
       " 'theguardian',\n",
       " 'theobserver',\n",
       " 'travel',\n",
       " 'travel-smarter',\n",
       " 'tv-and-radio',\n",
       " 'uk-news',\n",
       " 'unexpected-paris',\n",
       " 'us-news',\n",
       " 'virgin-australia-northern-lights',\n",
       " 'westpac-shaping-tomorrow',\n",
       " 'westpac-super-savers',\n",
       " 'world',\n",
       " 'world-vision-stand-with-girls',\n",
       " 'xero-small-changes-big-impact',\n",
       " 'you-can-adopt'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the sections list and then choose some sections for the subset of the dataset\n",
    "# then to choose some sections based on this list, other than the ones from the website\n",
    "len(sections)\n",
    "unique_sections = set(sections)\n",
    "len(unique_sections)\n",
    "unique_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/torkellingvarsson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6422"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# based on the section list, I add some more sections to the subset\n",
    "# \n",
    "idxes = []\n",
    "subtexts = []\n",
    "for i, section in enumerate(sections):\n",
    "    if section in ['news','world news', 'politics', 'society', 'world', 'global', 'cities', 'community', 'culture', 'uk-news']:\n",
    "        idxes.append(i)\n",
    "        subtexts.append(texts[i])\n",
    "len(idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6422x80722 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1894084 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second, CountVectorizer, creates document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords as sw\n",
    "model_vect = CountVectorizer(stop_words=sw.words('english'), token_pattern=r'[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "data_vect = model_vect.fit_transform(subtexts)\n",
    "data_vect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62307 79666 53209 37465 29691 50668  9352  2405 62941 15973]\n",
      "Top words: ['said', 'would', 'people', 'johnson', 'government', 'one', 'brexit', 'also', 'says', 'could']\n"
     ]
    }
   ],
   "source": [
    "# gives a random selection of terms\n",
    "import random\n",
    "random.sample(model_vect.get_feature_names(), 10)\n",
    "\n",
    "# prints the top indexes\n",
    "counts = data_vect.sum(axis=0).A1\n",
    "top_idxs = (-counts).argsort()[:10]\n",
    "top_idxs\n",
    "print(top_idxs)\n",
    "\n",
    "# prints the top words\n",
    "inverted_vocabulary = dict([(idx, word) for word, idx in model_vect.vocabulary_.items()])\n",
    "top_words = [inverted_vocabulary[idx] for idx in top_idxs]\n",
    "print(\"Top words: %s\" % top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mun,',\n",
       " 'Walpole,',\n",
       " 'BU',\n",
       " '5-21),',\n",
       " 'postdoctoral',\n",
       " 'tbe',\n",
       " 'Monroe.',\n",
       " '(p24).',\n",
       " 'Madagascar’s',\n",
       " 'Fearney',\n",
       " '5c',\n",
       " 'things…',\n",
       " 'jolt',\n",
       " '1.4sec.',\n",
       " 'Talladega,',\n",
       " 'Hyperbat,',\n",
       " 'woes..',\n",
       " 'Muses',\n",
       " '12-4-25-2.',\n",
       " 'Karume,']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random selection of terms\n",
    "import random\n",
    "random.sample(unique_words, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hong',\n",
       " 'kong',\n",
       " 'protests',\n",
       " 'masked',\n",
       " 'protest',\n",
       " 'extradition',\n",
       " 'china',\n",
       " 'police',\n",
       " 'protester']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = ['hong', 'kong', 'protests', 'masked', 'protest', 'extradition', 'china', 'police', 'protester']\n",
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2257, 2167, 1670, 69, 1022, 282, 1797, 7163, 165]\n"
     ]
    }
   ],
   "source": [
    "# then to see the term count of the query words:\n",
    "term_idxs = [model_vect.vocabulary_.get(term) for term in terms]\n",
    "term_counts = [counts[idx] for idx in term_idxs]\n",
    "print(term_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([62307, 37465, 9352, 79666, 53209, 29691, 17846, 54759, 52577, 39470],\n",
       " ['said',\n",
       "  'johnson',\n",
       "  'brexit',\n",
       "  'would',\n",
       "  'people',\n",
       "  'government',\n",
       "  'deal',\n",
       "  'police',\n",
       "  'party',\n",
       "  'labour'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 terms based on TF-IDF \n",
    "# >>\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "model_tfidf = TfidfTransformer()\n",
    "data_tfidf = model_tfidf.fit_transform(data_vect)\n",
    "data_tfidf\n",
    "\n",
    "freqs = data_tfidf.mean(axis=0).A1\n",
    "top_idxs = (-freqs).argsort()[:10].tolist()\n",
    "top_words = [inverted_vocabulary[idx] for idx in top_idxs]\n",
    "(top_idxs, top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.1116487664379395,\n",
       " 4.132850974088543,\n",
       " 3.415011180938226,\n",
       " 5.62450585086626,\n",
       " 3.4432816148764815,\n",
       " 4.5971565822196405,\n",
       " 3.5691618607654845,\n",
       " 2.3554802423125873,\n",
       " 5.040252758545452]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here it prints the weights for the selected terms\n",
    "idfs = model_tfidf.idf_\n",
    "term_idfs = [idfs[idx] for idx in term_idxs]\n",
    "term_idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 268,  961, 1077, ..., 3058, 3077, 3678]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# based on the selected terms, I make a query for those terms\n",
    "query = \" \".join(terms)\n",
    "query\n",
    "\n",
    "query_vect_counts = model_vect.transform([query])\n",
    "query_vect = model_tfidf.transform(query_vect_counts)\n",
    "query_vect\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sims = cosine_similarity(query_vect, data_tfidf)\n",
    "sims\n",
    "\n",
    "sims_sorted_idx = (-sims).argsort()\n",
    "sims_sorted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Victims of an attack by masked men in a Hong Kong train station on Sunday have accused police of not responding to their calls for help, saying: “We saved ourselves … because nobody else was there.” Their faces obscured by masks, they gave emotional testimony at a press briefing on Wednesday alongside the Democratic party lawmaker Lam Cheuk-ting, who was also injured in the attacks. Their statements came as Beijing blamed the unrest in Hong Kong on “black hands” from the US, advising America to remember that “Hong Kong is China’s Hong Kong”. One man, who gave his surname as Leung, was on his way to meet friends when he stopped at Yuen Long and saw the men attacking commuters on the platform and in train carriages. “The carriage was full of the smell of blood,” he said. He tried to help a man who was on the ground and was beaten over the head. “During the whole thing I didn’t see a single policeman or MTR [transport] staff,” he said, adding that the attack lasted 30 minutes. “I kept looking for the police but they never came.” A woman who gave her surname as Hui said she witnessed the attacks from inside her train carriage whose doors did not open when her train reached the station. She said she watched helplessly, calling the emergency services but no one answered. “We saved ourselves … because nobody else was there”, she said. Others stressed that those attacked were not just protesters. A 58-year-old man recounted in English how he arrived at the station with his son to find women screaming and asking for help and more than 100 men in white “attacking kids”. He described the 20 minutes he experienced there as “life and death”. “We are innocent citizens, passengers on the way home,” he said. “I am asking for global help for Hong Kong people. We don’t trust our government.” On Sunday, dozens of men in white T-shirts attacked commuters, some of whom were returning from a mass anti-government march that day. The attack and delayed response by police has led to accusations of government collusion with organised crime groups active in those areas. The attacks have added new momentum to protests, which began over a controversial extradition bill and have now taken on new demands, including an investigation into police behaviour. On Wednesday protesters temporarily blocked morning commuters at a major transit station in Admiralty, in central Hong Kong, as they demanded answers from the transportation authority. Several other rallies are planned for the week and weekend. Earlier, China’s ministry of foreign affairs blamed foreign forces, and in particular the US, for the turmoil in Hong Kong. “We can see that US officials are behind such incidents. Can US officials honestly tell the world what role they played and what are their aims?” spokeswoman Hua Chunying said on Tuesday. “We advise the US to withdraw their black hands,” she said. “The US should know one thing, that Hong Kong is China’s Hong Kong and we do not allow any foreign interference,” she said. On Wednesday, the People’s Daily, the official mouthpiece of the ruling Chinese Communist party, echoed Hua’s comments, adding: “Violent incidents have happened in Hong Kong and throughout it all the US has encouraged it. Some Americans say they want to help Hong Kong, but really they want to bring chaos to Hong Kong and will not be satisfied until Hong Kong has collapsed.” Weeks of protests that have been focused primarily on the Hong Kong government took a more violent turn on Sunday, when protesters targeted Beijing’s representative office in Hong Kong, defacing the national emblem of the People’s Republic of China with black paint. Since then, Beijing has stepped up its rhetoric, publishing a series of editorials condemning the protesters and allowing images and news reports of the vandalism to spread on social media, where almost all content related to the Hong Kong protests had been previously been censored. For the past two months, Chinese state media have blamed the protests on “foreign interference” and singled out those who have issued statements on Hong Kong, such as the US House Speaker, Nancy Pelosi, and the UK foreign secretary, Jeremy Hunt. By singling out the US, China is threatening to escalate already high tensions between the US and China. Beijing’s comments come after the US president, Donald Trump, applauded the Chinese leader, Xi Jinping, for “acting very responsibly” in regards to Hong Kong. “China could stop them if they wanted,” Trump said on Monday of the protests. “Those are big protests … I hope that President Xi will do the right thing, but it has been going on a long time, there’s no question.” On Tuesday, the US senator Marco Rubio condemned the suspected use of hired thugs to go after protesters at Yuen Long train station on Sunday. “People that put in power thugs and criminals and gangsters to go after innocent people and beat them the way they did … the whole world should be condemning it. They can now see the true nature of that government of that Communist party,” he said. Some observers believe China’s escalated language and circulation of news of the “riots” in Hong Kong may be preparation for stronger actions against the city. Chinese leaders also blamed “foreign interference” for pro-democracy protests in China in 1989, before cracking down on demonstrators. Wu Qian, a spokesman for China’s defence ministry, said on Wednesday his government was “closely following the developments in Hong Kong”. When asked whether the Chinese military, the People’s Liberation Army (PLA), could be deployed to Hong Kong, he said there were “clear stipulations” in Hong Kong’s army law, which said Hong Kong could ask for assistance from the PLA’s garrison in Hong Kong. Beijing has given no indication it plans to send in PLA troops. An editorial in the People’s Daily earlier this week said: “We firmly support the Hong Kong government in taking all necessary measures to ensure the safety of the central government agencies in Hong Kong, safeguard the rule of law in Hong Kong, and punish all criminals.” Additional reporting by Verna Yu and Lillian Yang'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtexts[sims_sorted_idx[0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
